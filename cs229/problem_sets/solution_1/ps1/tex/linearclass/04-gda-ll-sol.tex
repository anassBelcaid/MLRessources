\begin{answer}
  Let's compute the derivative of the log-likelihood of the data with respect to
  the prior $\phi$.

  \begin{eqnarray}
    \dfrac{\partial l}{\partial \phi} &=& 0\\
    \dfrac{\partial l}{\partial \phi}\big(\sum_{i=1}^n y^i\log(\phi) +
  (1-y^i)\log(1-\phi) \big) &=& 0\\
  \sum_{i=1}^n y^i\dfrac{1}{\theta} -(1-y^i)\dfrac{1}{(1-\theta)}&=&0\\
  \sum_{i=1}^n y^i &=&\theta
  \end{eqnarray}


We repeat the same work on the conditional means $u_1$ and $u_0$.
\newcommand{\precision}{\Sigma^{-1}}
\begin{eqnarray}
 \dfrac{\partial}{\partial
    u_0}\Big[\sum_{i=1}^n \mathbb{1}(y^i=0)
  -\dfrac{1}{2}(x-\mu_0)^T\precision(x-\mu_0)\Big]&=&0\\
  \sum_{i=1}^n \dfrac{\partial}{\partial \mu_0} 1(y^i=0)\big(x^T \precision \mu_0 -
\dfrac{1}{2}\mu_0^T\precision\mu_0\big)&=&0\\
\end{eqnarray}
Since $\precision$ is regular, we get

\begin{equation}
  \mu_0  = \dfrac{\sum_{i=1}^n 1(y^i=0)x^i}{\sum_{i=1}^n 1(y^i=0)}
\end{equation}


Finally for the precision matrix, we mention an important point that the
derivative can be from either the quadratic term but also the normalizing factor
the depends on the determinant of $\Sigma$.

\begin{eqnarray}
  \dfrac{\partial l}{\partial \precision} &=&\sum_{i=1}^n (x-\mu_i)^T(x-\mu_i) -
  \Sigma\\
\end{eqnarray}

The term $\Sigma$, is obtained by computing the derivative of the log of the
determinant.

Hence, we obtain:

\begin{equation}
  \Sigma =\dfrac{1}{n} \sum_{i=1}^n (x^i - \mu_{i})^T(x^i- \mu_{i})
\end{equation}


\end{answer}
